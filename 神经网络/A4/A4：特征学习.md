## A4：特征学习

姓名：陈俊卉     学号：2020212256     班级：2020219111

[TOC]

#### **任务1. PCA or kernel PCA (10 points)**

- 使用PCA或kernel PCA对手写数字数据集[MINST](http://yann.lecun.com/exdb/mnist/)进行降维。观察前两个特征向量所对应的图像，即将数据嵌入到R2空间。绘制降维后的数据，并分析二维特征是否能够足以完成对输入的分类，对结果进行分析和评价。

##### 1、降维并测试分类效果

使用 sklearn.decomposition 内的PCA对 sklearn.datasets 内的MINST数据集进行降维（预先下载好mat包）。

该数据集一共有70000个数据，每个数据有784维：

```python
mnist = loadmat('C://Users/HaRry_/Desktop/神经网络/A4/mnist-original.mat')
X = mnist['data'].T
y = mnist['label'].T.flatten()
print(X.shape)
```

```
(70000, 784)
```

随后使用pca将数据降到二维后画出$R_2$空间的二维图像。绘制完成如下图所示：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130150743444.png" alt="image-20221130150743444" style="zoom: 67%;" />

可以看出，只有类别为1的数据能被较好地分类，其他数据点都夹杂在一起。

我们借用sklearn.decomposition 内的PCA自带的指标 **pca.explained_variance_ratio_**，可以看出降维后的每一个主成分解释原始数据的方差的相应比例。

对降到二维后的数据输出 **pca.explained_variance_ratio_**：

```python
print(pca.explained_variance_ratio_)
print(sum(pca.explained_variance_ratio_))
```

输出如下：

```
[0.09746116 0.07155445]
0.16901560501321666
```

可以看出所占的比例很少，说明分类效果会比较差。

再通过 **sklearn.neighbors** 的KNN测试降维前后的分类效果，并测试所需时间（单位：秒）：

![image-20221130152337219](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130152337219.png)

可以看出，如果不降维，得到的结果为0.9688，分类效果比较好，但耗时相对很长；而降维到二维虽然时间很短，但分类效果很差。

**所以二维不能完成对输入的分类。**

##### 2、寻找合适的降维维度

先将所有维度输入到PCA，查看所有主成分的方差占原始数据总方差的百分比：

```python
pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
print(sum(pca.explained_variance_ratio_))
plt.plot([i for i in range(X_train.shape[1])],
    [np.sum(pca.explained_variance_ratio_[:i+1]) for i in range(X_train.shape[1])])
plt.show()
```

输出：

```
0.9999999999999994
```

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130153527124.png" alt="image-20221130153527124" style="zoom: 67%;" />

**为了探究使用多少维能够又快又好地分辨数据：遍历1~600维度的score和time:**

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130232412353.png" alt="image-20221130232412353" style="zoom:67%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130232619597.png" alt="image-20221130232619597" style="zoom:67%;" />

**对照上面两图可以看出，选取10维左右就可以获得较高的准确率且所需时间很短。**





#### **任务2. Autoencoder (10 points)**

- 使用自动编码器学习输入的特征表示。尝试设计一个全链接前馈神经网络。尝试使用不同的损失函数和正则化方法。

##### 1、网络结构

```python
class auto_encoder(torch.nn.Module):
    def __init__(self):
        super(auto_encoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 512),
            nn.Tanh(),
            nn.Linear(512, 256),
            nn.Tanh(),
            nn.Linear(256, 128),
            nn.Tanh(),
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 4),
        )

        self.decoder = nn.Sequential(
            nn.Linear(4, 32),
            nn.Tanh(),
 	        nn.Linear(32, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Tanh(),
            nn.Linear(128, 256),
            nn.Tanh(),
            nn.Linear(256, 512),
            nn.Tanh(),
            nn.Linear(512, 28 * 28),
            nn.Sigmoid()
        )
```

- 采用encoder和decoder对称的形式
- encoder用来编码提取特征，decoder用作解码还原



##### 2、训练

采用多种不同的loss函数进行训练，其中采取最优秀的损失函数使用不同的正则化方式。

使用tensorboard记录loss数据。

###### ① MSEloss

- train loss：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130185959526.png" alt="image-20221130185959526" style="zoom:67%;" />

- test loss：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130190051239.png" alt="image-20221130190051239" style="zoom:67%;" />

- evaluate：

  - epoch = 100：

    - encoder后使用t-SNE进行表示：

    - <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130192718487.png" alt="image-20221130192718487" style="zoom: 33%;" />

    - 原图与decoder之后的比对：

      <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130193017354.png" alt="image-20221130193017354" style="zoom:50%;" />

  可以发现，encoder的特征提取效果较好，相同的数字能较好地聚类，但仍存在些许混淆的情况；t-SNE和比对图都可以很明显地看出4和9的混淆。

  epoch = 100可能有点过拟合，考虑使用epoch较小的ckpt进行evaluate.

  - epoch = 50：

    - encoder后使用t-SNE进行表示：

      <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130193639577.png" alt="image-20221130193639577" style="zoom:33%;" />

    - 原图与decoder之后的比对：

      <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130193737807.png" alt="image-20221130193737807" style="zoom:50%;" />

t-SNE效果有所提升，而比对没有什么改善。



###### ② L1Loss

- test loss:

  <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130214632117.png" alt="image-20221130214632117" style="zoom:67%;" />

- train loss:

  <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130214730052.png" alt="image-20221130214730052" style="zoom:67%;" />

- evaluate:

  - epoch = 100:

    - encoder后使用t-SNE进行表示：

      <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130211716807.png" alt="image-20221130211716807" style="zoom: 33%;" />

    - 原图与decoder之后的比对：

      <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130220141179.png" alt="image-20221130220141179" style="zoom:50%;" />

效果比使用MSEloss要好，且基本没有出现数字判断错误的情况，但某些特征丢失了。

###### ③ SmoothL1Loss

- test loss:

  <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130225640193.png" alt="image-20221130225640193" style="zoom:67%;" />

- train loss:

  <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130225703392.png" alt="image-20221130225703392" style="zoom:67%;" />

- evaluate

  - epoch = 100

    - encoder后使用t-SNE进行表示：

      ![image-20221130225754174](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130225754174.png)

    - 原图与decoder之后的比对：

      ![image-20221130225945409](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221130225945409.png)

t-SNE看起来要比前两个更加好，但还原之后并没有前两个那么清晰。



##### 3、使用SmoothL1Loss，采用不同的正则化方法

###### ① L1正则化

练了20epoch，loss就收敛了，但是基本没学到：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201005417968.png" alt="image-20221201005417968" style="zoom:33%;" />



###### ② L2正则化

- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201005229407.png" alt="image-20221201005229407" style="zoom: 67%;" />
- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201005249746.png" alt="image-20221201005249746" style="zoom:67%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201005322020.png" alt="image-20221201005322020" style="zoom:33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201005215176.png" alt="image-20221201005215176" style="zoom:50%;" />



**无法收敛。**

这可能是优化器的问题。

**将系数调小再尝试：**

**L1正则化：**仍然无法收敛。原因很有可能是因为**L1正则化使得解空间变得比较不光滑，难以收敛到一个局部最优解。**

**L2正则化：**在*正则化系数减小到1e-6*后收敛：

- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201030520012.png" alt="image-20221201030520012" style="zoom:67%;" />
- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201030539713.png" alt="image-20221201030539713" style="zoom:67%;" />
- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201030629151.png" alt="image-20221201030629151" style="zoom:33%;" />
- <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201030649506.png" alt="image-20221201030649506" style="zoom:50%;" />

**聚类效果有了质的飞跃。**除零星的点之外，所有数字基本都聚成了一类。而与原图的比对也只有一个错误，其他都是高质量还原。**这对于全链接前馈神经网络来说是一个较为优秀的结果。**同时train loss和test loss都降到了一个之前都没有达到过的值。



| train loss | test loss |
| :--------: | :-------: |
|    5e-6    |   5e-5    |



#### **附加题** (**BONUS: 10 points)**

- 模型训练中，你可以尝试任何可以提升模型性能的合理的方法。例如其它的网络结构、设计多个隐藏层、引入降噪自动编码器等任何你能想到的方法。计算模型在训练集和测试集上的损失，并对结果进行讨论。

##### 1、在数据集添加噪声，试图训练降噪自动编码器

使用

```
torch.randn_like(inputs)
```

对原有的图像增加噪声，**以期望encoder提取的特征更能代表数字本身，decoder能还原原来无噪声数字的样子。**

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201130601113.png" alt="image-20221201130601113" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201130612617.png" alt="image-20221201130612617" style="zoom:80%;" />

| train loss | test loss |
| :--------: | :-------: |
|  5.86e-6   |  3.32e-5  |

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201131042482.png" alt="image-20221201131042482" style="zoom:33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201131132561.png" alt="image-20221201131132561" style="zoom:50%;" />



**可以看到拟合效果有明显改善，类与类之间的距离变大了（这意味着不同类之间更能相互区分），decoder后数字形态基本一致！这对于全连接前馈神经网络来说已经比较优秀了。而对数据集增加噪声本身也是一种正则化，让模型有了更加好的泛化性能和特征提取能力。**



##### 2、其它的网络结构：在全连接前馈神经网络前增加卷积操作，相应调整全连接网络的输入、输出

在encoder和decoder对称加入以下卷积：

```python
          nn.Conv2d(1, 8, 3, stride=2, padding=1),
          nn.Tanh(),
          nn.Conv2d(8, 16, 3, stride=2, padding=1),
          nn.BatchNorm2d(16),
          nn.Tanh(),
          nn.Conv2d(16, 32, 3, stride=2, padding=0),
          nn.Tanh()
```



###### ① 使用MSELoss

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201215811777.png" alt="image-20221201215811777" style="zoom: 80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201215901193.png" alt="image-20221201215901193" style="zoom:80%;" />

 	<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201215934214.png" alt="image-20221201215934214" style="zoom: 33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201220006560.png" alt="image-20221201220006560" style="zoom:50%;" />

| train loss | test loss |
| :--------: | :-------: |
|  0.04819   |  0.03571  |



**由图可知，聚类效果显然比仅仅使用全连接网络要更好：类与类之间的距离普遍较大，而类内距离普遍较短。其甚至可以将一些原本写得不规范的数字转为规范的数字，真正具有了提取数字特征，还原原先数字的能力。也说明了卷积本身比全连接网络更加能够提取数据特征。**



###### ② 使用SmoothL1Loss

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201234006258.png" alt="image-20221201234006258" style="zoom: 80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201234020659.png" alt="image-20221201234020659" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201233846176.png" alt="image-20221201233846176" style="zoom:33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221201234043807.png" alt="image-20221201234043807" style="zoom:50%;" />

| train loss | test loss |
| :--------: | :-------: |
|  0.02423   |  0.01882  |

**类间距离相比于使用MSELoss分的更开；在encode-decode后能够矫正数字的一些不正常的形状与笔画的同时还能保留原来字的书写特征，但类间距离也相应地增大了。**





###### ③ 将Adam更换为SGD

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202002819295.png" alt="image-20221202002819295" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202002834117.png" alt="image-20221202002834117" style="zoom: 80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202002857706.png" alt="image-20221202002857706" style="zoom:33%;" />

模型收敛在了一个奇怪的很差的解上，**这可能与数据的分布不太适合有关。**



###### ④ 先用Adam收敛，再使用SGD寻找最优解

该想法来源于 [Improving Generalization Performance by Switching from Adam to SGD](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1712.07628)：

*Adam的收敛速度比SGD要快，但最终收敛的结果并没有SGD好。主要是因为后期Adam的学习率太低，影响了收敛的效果。他们提出了一个用来改进Adam的方法：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。*

- Adam收敛

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202012853508.png" alt="image-20221202012853508" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202012901936.png" alt="image-20221202012901936" style="zoom:80%;" />



- 使用SGD寻找最优解（学习率调节到1e-7）

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202171150957.png" alt="image-20221202171150957" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202171239598.png" alt="image-20221202171239598" style="zoom:80%;" />

观察图像，可以知道不仅没有找到最优解，test loss没有收敛，而且还过拟合了...观察下列结果也可以验证这个事实：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202171325533.png" alt="image-20221202171325533" style="zoom:33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202171357897.png" alt="image-20221202171357897" style="zoom:50%;" />

由于此次实验没有收敛，故没有给出loss值。

合理猜测：SGD不太适合运用在该任务中作为第一个优化器；笔者没有掌握好Adam到SGD切换的时机和切换后的学习率调整。



###### ⑤ 在最优卷积降噪autoencoder的基础上将种子设置为3407

- 使用MSELoss

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202191335807.png" alt="image-20221202191335807" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202191453248.png" alt="image-20221202191453248" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202191750065.png" alt="image-20221202191750065" style="zoom:33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221202191811421.png" alt="image-20221202191811421" style="zoom:50%;" />

| train loss | test loss |
| :--------: | :-------: |
|  0.04823   |  0.03551  |

**类间距离再一次被拉开一些，但基本没有用类内距离作为代价。这说明随机有效果。**



- 使用SmoothL1Loss

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221203075934545.png" alt="image-20221203075934545" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221203075949325.png" alt="image-20221203075949325" style="zoom:80%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221203080300696.png" alt="image-20221203080300696" style="zoom: 33%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221203080318906.png" alt="image-20221203080318906" style="zoom:50%;" />

| train loss | test loss |
| :--------: | :-------: |
|  0.02421   |  0.01885  |

虽然与之前相比，train loss和 test loss都降低了一点，但分类效果反而没有之前那么好；但loss曲线比之前平滑。这说明更换随机种子确实能够让收敛结果改变，但不一定都往好的方向发展，需要综合调参。



#### **总结**

- #### 本次实验，笔者完成了：
  
  - **任务1.**使用PCA或kernel PCA对手写数字数据集[MINST](http://yann.lecun.com/exdb/mnist/)进行降维。观察前两个特征向量所对应的图像，即将数据嵌入到R2空间。绘制降维后的数据，并分析二维特征是否能够足以完成对输入的分类，对结果进行分析和评价。
    
    - extra：探究了将数据降到多少维能够在不太损失精度的情况下识别出数字并使用更少的时间
  - **任务2.** 使用自动编码器学习输入的特征表示。尝试设计一个全链接前馈神经网络。尝试使用不同的损失函数和正则化方法。
    - MSELoss
    - L1Loss
    - SmoothL1Loss
    - L1正则化（调参仍无法收敛）
    - L2正则化
    - 对L2正则化进行调参（取得很好效果）
  - **附加题.**模型训练中，你可以尝试任何可以提升模型性能的合理的方法。例如其它的网络结构、设计多个隐藏层、引入降噪自动编码器等任何你能想到的方法。计算模型在训练集和测试集上的损失，并对结果进行讨论。
    - 引入噪声，试图训练出降噪autoencoder（work：类间距离变大）
    
    - 网络结构中引入卷积，以更彻底地提取特征（work：类间距离变大，类内距离变小）
      - 尝试MSELoss
      - 尝试SmoothL1Loss（比MSELoss效果更好）
      
    - 将Adam换为SGD（没有收敛）
    
    - 先使用Adam收敛，再使用SGD降低学习率试图找到最优解）（没有更好的效果，过拟合）
    
    - seed(3407)（曲线变得更加平滑，但分类效果不一定变得更好）
      - MSELoss（work）
      - SmoothL1Loss（do not work）
      
    - 最佳超参数：
    
      - ```python
        batch_size=64
        lr= 0.001  # 尝试了0.001、0.005、0.01
        seed(1)    # 尝试了 1、2、3、3407
        weight_decay=1e-06
        optim = torch.optim.Adam()
        epoch = 150  #(200过拟合、100欠拟合)
        ```
    
    - 最佳loss：SmoothL1Loss
  
- #### 学到的地方：

  - 笔者对特征学习的理解更加深刻，对调参的方法更加熟悉；对使用pytorch写网络训练更加得心应手。
  - 维度并不是越高越好，恰当的维度可以加速且达到较高的准确率。
  - 当使用正则化时，不要想当然的以为自己正则化的程度已经很小了（如尝试了1e-5仍然无法收敛之后就停止再往小尝试），有可能在更小的程度能收敛且达到更优秀的效果。
  - 添加噪声是一个很好的增加模型泛化能力的方式。
  - 如果线性层提取图像信息的效果不特别好，那么可能是某些空间信息存在难以提取的问题，可以尝试增加卷积提取信息。
  - 一个损失函数在某一方面可能是在所有模型的测试中综合起来最好的损失函数，但在单一的某个模型可能另一个损失函数更优。要多加尝试。
  - 虽然没有成功证明先用Adam再用SGD可以很好的训练收敛找到最优解，但这个方法值得学习。

