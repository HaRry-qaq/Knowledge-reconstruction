## A2：分析与简答

姓名：陈俊卉     班级：2020219111      学号：2020212256

[TOC]



#### A2.1 以一个简单的1-1-1结构的两层神经网络为例,分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）

##### Answer:

由于两层网络要求关于参数$w_1$与$w_2$的二阶导是比较困难的，所以我们尝试画图解释。

##### 对于均方误差损失函数：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194539057.png" alt="image-20221128194539057" style="zoom:80%;" />

为了探究关于$w_1$、$w_2$是否非凸，我们令$x = k_1$

###### ① 令$w_1$为一常数，求$w_2$对$J$的函数是否非凸：

**y = 1:**

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194036426.png" alt="image-20221128194036426" style="zoom: 67%;" />

- $k_1$为正数、$w_1$为正数时：
  - ![image-20221128194204094](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194204094.png)



- $k_1$为负数、$w_1$为正数时：
  - ![image-20221128194251478](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194251478.png)



- $k_1$为正数、$w_1$为负数时：
  - ![image-20221128194348976](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194348976.png)



- $k_1$为负数、$w_1$为负数时：
  - ![image-20221128194448955](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128194448955.png)



**y = 0：**

- $k_1$为正数、$w_1$为正数时：
  - ![image-20221128195242265](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128195242265.png)



- $k_1$为负数、$w_1$为正数时：
  - ![image-20221128195310148](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128195310148.png)



- $k_1$为正数、$w_1$为负数时：
  - ![image-20221128195344466](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128195344466.png)



- $k_1$为负数、$w_1$为负数时：
  - ![image-20221128195432171](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128195432171.png)

**有非凸情况。**





###### ② 令$w_2$为一常数，求$w_1$对$J$的函数是否非凸：

**y=1:**

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200218851.png" alt="image-20221128200218851" style="zoom: 67%;" />



- $k_1$为正数、$w_2$为正数时：
  - ![image-20221128200326137](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200326137.png)



- $k_1$为负数、$w_2$为正数时：
  - ![image-20221128200358947](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200358947.png)



- $k_1$为正数、$w_2$为负数时：
  - ![image-20221128200437612](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200437612.png)



- $k_1$为负数、$w_2$为负数时：
  - ![image-20221128200500026](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200500026.png)

**y=0:**

- $k_1$为正数、$w_2$为正数时：
  - ![image-20221128200537308](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200537308.png)



- $k_1$为负数、$w_2$为正数时：
  - ![image-20221128200558321](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200558321.png)



- $k_1$为正数、$w_2$为负数时：
  - ![image-20221128200622332](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200622332.png)



- $k_1$为负数、$w_2$为负数时：
  - ![image-20221128200645106](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128200645106.png)

**有非凸情况。**



##### 对于交叉熵损失：

![image-20221128203037415](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128203037415.png)

**y=1:**

###### ① 令$w_2$为一常数，求$w_1$对$J$的函数是否非凸：

- $k_1$为正数、$w_2$为正数时：
  - ![image-20221128204055929](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204055929.png)
- $k_1$为负数、$w_2$为正数时：
  - ![image-20221128204114891](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204114891.png)
- $k_1$为正数、$w_2$为负数时：
  - ![image-20221128204156260](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204156260.png)
- $k_1$为负数、$w_2$为负数时：
  - ![image-20221128204220679](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204220679.png)



###### ② 令$w_1$为一常数，求$w_2$对$J$的函数是否非凸：

- $k_1$为正数、$w_1$为正数时（$k_1$为负数、$w_1$为负数时）：
  - ![image-20221128204613797](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204613797.png)
- $k_1$为负数、$w_1$为正数时（$k_1$为正数、$w_1$为负数时）：
  - ![image-20221128204712855](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204712855.png)



**y=0：**

###### ① 令$w_1$为一常数，求$w_2$对$J$的函数是否非凸：

- $k_1$为正数、$w_1$为正数时（$k_1$为负数、$w_1$为负数时）：
  - ![image-20221128204824685](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204824685.png)
- $k_1$为负数、$w_1$为正数时（$k_1$为正数、$w_1$为负数时）：
  - ![image-20221128204842610](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204842610.png)



###### ② 令$w_2$为一常数，求$w_1$对$J$的函数是否非凸：

- $k_1$为正数、$w_2$为正数时：
  - ![image-20221128204951778](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128204951778.png)
- $k_1$为负数、$w_2$为正数时：
  - ![image-20221128205009462](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128205009462.png)
- $k_1$为正数、$w_2$为负数时：
  - ![image-20221128205031297](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128205031297.png)
- $k_1$为负数、$w_2$为负数时：
  - ![image-20221128205052107](C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128205052107.png)



**有非凸情况，也有凸的情况。**



**此外，可以试着画出mse与ce函数的三维图像：**

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128212508058.png" alt="image-20221128212508058" style="zoom:67%;" /><img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128212942297.png" alt="image-20221128212942297" style="zoom: 67%;" />



由上述两种作图方法可以看出：

- 关于w1，无论是均方误差损失函数还是交叉熵损失函数，y=1和y=0都是存在非凸的；
- 关于w2，均方误差对于y=1和y=0都是存在非凸的；而交叉熵损失函数对于y=1和y=0都是凸的。





#### A2.2 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然

##### Answer:

回归问题中，线性假设目标值可以表示为特征的加权和：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127215626336.png" alt="image-20221127215626336" style="zoom: 50%;" />

使用均方误差损失函数来衡量观察值 和预测值之间的差距：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127215855155.png" alt="image-20221127215855155" style="zoom: 50%;" />

而均方误差损失函数可以用于线性回归的一个原因是我们假设观察值中包含服从正态分布的噪声：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127215955771.png" alt="image-20221127215955771" style="zoom: 67%;" />

给定x，则y的似然函数为：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127231039037.png" alt="image-20221127231039037" style="zoom:67%;" />

则根据极大似然估计，参数w和b的最优值是使得整个数据集似然最大的值：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127231256357.png" alt="image-20221127231256357" style="zoom:67%;" />

则负对数似然为

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127231346436.png" alt="image-20221127231346436" style="zoom: 67%;" />

显然，最大化似然等价于最小化对数似然。

由于$\sigma$ 是一个固定值，则第一项为常数，$\sigma^2$也为常数。所以最小化负对数似然等价于均方误差：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127231527088.png" alt="image-20221127231527088" style="zoom:67%;" />

$Q.E.D.$



#### A2.3 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然

##### Answer:

###### **① 对于无标签样本**：

约定：

- 数据的真实分布：<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234458462.png" alt="image-20221127234458462" style="zoom: 67%;" />

- 模型的近似分布：<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234544097.png" alt="image-20221127234544097" style="zoom:67%;" />或<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234602627.png" alt="image-20221127234602627" style="zoom:67%;" />

则极大似然估计：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234723084.png" alt="image-20221127234723084" style="zoom:67%;" />

由于x从<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234458462.png" alt="image-20221127234458462" style="zoom: 67%;" />采样得来，根据大数定理，所以有

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234833052.png" alt="image-20221127234833052" style="zoom: 67%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127234953761.png" alt="image-20221127234953761" style="zoom:67%;" />

则得到交叉熵损失函数。所以极大似然和最小化交叉熵损失等价。



###### **② 对于有标签样本**：

约定：

- 数据的真实分布：<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235306896.png" alt="image-20221127235306896" style="zoom: 80%;" />

- 模型的近似分布：<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235343713.png" alt="image-20221127235343713" style="zoom:67%;" />或<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235400455.png" alt="image-20221127235400455" style="zoom:67%;" />

则极大似然估计：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235454152.png" alt="image-20221127235454152" style="zoom:67%;" />

因为x、y是从<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235535849.png" alt="image-20221127235535849" style="zoom:67%;" />采样得来，同①，有

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235622195.png" alt="image-20221127235622195" style="zoom:67%;" />

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221127235706658.png" alt="image-20221127235706658" style="zoom:67%;" />

$Q.E.D.$



#### A2.4 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解

##### Answer 1:图解法

###### ① L1正则化

对于2维数据，L1范数为$|x|+|y|$ .对于$|x|+|y|=c$（c为常数），我们可以画出以下图像： 

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128002805017.png" alt="image-20221128002805017" style="zoom: 67%;" />

而在“尖点”，解就是稀疏的，因为有某一维度的值为0.

推广到n维的情况，L1范数所代表的形状应该为一个有许多“棱角”的立体。而在这些棱角，就是系数解。

若没有正则化项，则学到的模型是最矮等高线代表的模型，即最里层的等高线。加上正则化后，模型参数受到了约束。此时若还是取经验风险最小的模型，则很难满足正则化项的约束条件，因此需要找到合适的参数，使得满足约束条件的同时经验风险最小。

将L1正则化加入损失函数：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128003225916.png" alt="image-20221128003225916" style="zoom:67%;" />

相当于把w的解限定在图形内（正则化参数决定图形大小）的同时使得经验损失最小：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128004636148.png" alt="image-20221128004636148" style="zoom:50%;" />

事实上，我们可以直观的感受到，棱角触碰到最优解的可能性是很大的：

假设当前优化状态在点A，可以继续优化，经验损失的优化方向为图中所示的$\triangle l_a$,而参数$W$必须满足约束条件，即参数需在菱形上，因此参数会沿着$\triangle l_a$.在菱形方向的分量优化，使得参数向顶点靠近，当到达顶点B所在状态时，达到经验损失最小，即绿色等高线表示的损失。可以看到B点所在的状态的损失优化方向$\triangle l_b$虽然也会产生菱形方向的分量，**但如果此时沿着分量继续则会导致经验损失增大**。顶点B点所在的状态，参数$W$某个维度为0，因此$L_1$正则化容易使模型参数稀疏。

###### ② L2正则化

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128005230126.png" alt="image-20221128005230126" style="zoom:50%;" />

同理，当状态沿着圆形变化时，会得到一个经验损失最小的点B，此时经验损失优化方向已经没有产生切线方向的分量了。所以此时B点就是经验损失最小的状态。而只有当经验损失中心最优解在坐标轴上时，才会取到稀疏解。且这样的点一般离某个坐标轴很近，因此$L_2$正则化不容易使模型参数稀疏但可以减小模型参数。



##### Answer 2:求导法

对L1正则化损失函数求导得到参数更新：

$w \rightarrow w^{'}=w - \frac{\eta \lambda}{n}sgn(w)-\eta \frac{\partial C_0}{\partial w}$

对L2正则化损失函数求导得到参数更新：

$w \rightarrow w^{'}=w - \frac{\eta \lambda}{n}w-\eta \frac{\partial C_0}{\partial w}$

当$w\in[1,+\infty)$,L2获得比L1更快的减小速率；

当$w\in(0,1)$,L1获得比L2更快的减小速率，且$w$越小，L1更容易接近于0，而L2更不容易变化。

因此L1会获得比L2更多的稀疏解。



##### Answer 3:先验概率分布法

与L1正则化对应的是拉普拉斯分布（拉普拉斯分布的概率密度取对数就是L1范式）：

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128012334474.png" alt="image-20221128012334474" style="zoom: 33%;" />

从图中可以看到，只有很小的概率有值,大部分概率值都很小或为0。这就是稀疏的。

而与L2正则化对应的是正态分布。

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128012552013.png" alt="image-20221128012552013" style="zoom:50%;" />

高斯分布中，参数在0值附近分布成平滑凸起状，因此在0值附近取到的概率很大。

这也解释了为什么L1正则化容易使参数稀疏而L2正则化容易使参数减小到接近0。



#### A2.5 分析Batch normalization对参数优化起到什么作用、如何起到这种作用

##### 1、Batch normalization的过程

如下图所示。

<img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128013803756.png" alt="image-20221128013803756" style="zoom:50%;" />

##### 2、作用与原因

其主要解决

- 加快收敛速度
  - 在DNN中，若每一层的数据分布不一，网络训练难度会很大。而若把每一层的数据都归一化，每一层数据都转换为均值为0，方差为1的状态，训练收敛速度会加快。
- 防止梯度爆炸和梯度损失，让大部分的激活函数能够远离其饱和区域
  - 梯度损失：以$sigmoid$函数为例，当网络的激活输出很大，其对应的梯度就会很小，导致网络的学习速率就会很慢。通过链式传播，导致浅层的学习参数变得很小，基本不学习，而深层的网络则比较容易学习。这样深层的网络基本相当于整个网络，浅层网络失效，则深度失去了意义。而通过Batch normalization后，网络输出不会很大，梯度不会很小。
  - 梯度爆炸：当每一层的激活层斜率*权值大于1，且网络很深时，梯度会指数增加。浅层网络会出现梯度爆炸的情况。
- 防止过拟合
  - 在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个batch的其他样本，一定程度上避免了过拟合。
- 提高训练时模型对于不同超参的鲁棒性
- 根据文章https://arxiv.org/abs/1805.11604：
  - BN与ICS无关（加了noisy BN和标准的BN在training accuracy和收敛速度上几乎没有差异，并且都优于不加BN的方法）
    - <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128015650171.png" alt="image-20221128015650171" style="zoom: 33%;" />
  - BN重新改变了优化问题，使得优化空间变得非常平滑
    - <img src="C:\Users\HaRry_\AppData\Roaming\Typora\typora-user-images\image-20221128015742341.png" alt="image-20221128015742341" style="zoom: 33%;" />
  - 但BN并不是唯一能够使得优化空间变平滑的方法。一些其他方法：
    - $L_1$
    - $L_2$
    - $L_\infty$

